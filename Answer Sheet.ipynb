{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa0f033",
   "metadata": {},
   "source": [
    "# First we will load our json file in python and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c18068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"algoparams_from_ui.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f5ec3",
   "metadata": {},
   "source": [
    "# will import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c9f9a",
   "metadata": {},
   "source": [
    "# Q1  Read the target variable and type of regression to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa82aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"design_state_data\"][\"target\"][\"target\"]\n",
    "prediction_type = data[\"design_state_data\"][\"target\"][\"type\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f0024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0e3755",
   "metadata": {},
   "source": [
    "# Q2 Read the features (which are column names in the csv) and figure out what missing imputation needs to be applied and apply that to the columns loaded in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data[\"design_state_data\"][\"session_info\"][\"C:\\Users\\Ak7shy\\Desktop\\iris.csv\"])\n",
    "\n",
    "for feature, details in data[\"design_state_data\"][\"feature_handling\"].items():\n",
    "    if details[\"missing_values\"] == \"Impute\":\n",
    "        if details[\"impute_with\"] == \"Average of values\":\n",
    "            df[feature] = df[feature].fillna(df[feature].mean())\n",
    "        else:\n",
    "            df[feature] = df[feature].fillna(details[\"impute_value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdbb608",
   "metadata": {},
   "source": [
    "# Q3 Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA. Please make sure you write code so that all options can work. If we rerun your code with a different Json it should work if we switch No Reduction to say PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = data[\"design_state_data\"][\"feature_reduction\"][\"feature_reduction_method\"]\n",
    "if reduction_method == \"Tree-based\":\n",
    "    num_of_features_to_keep = int(data[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"])\n",
    "    num_of_trees = int(data[\"design_state_data\"][\"feature_reduction\"][\"num_of_trees\"])\n",
    "    depth_of_trees = int(data[\"design_state_data\"][\"feature_reduction\"][\"depth_of_trees\"])\n",
    "    \n",
    "    # Apply tree-based feature selection\n",
    "    selector = SelectKBest(f_regression, k=num_of_features_to_keep)\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_reduced = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    df = pd.concat([pd.DataFrame(X_reduced, columns=selected_features), df[target]], axis=1)\n",
    "elif reduction_method == \"PCA\":\n",
    "    num_of_features_to_keep = int(data[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"])\n",
    "    pca = PCA(n_components=num_of_features_to_keep)\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    selected_features = [f\"PC{i+1}\" for i in range(num_of_features_to_keep)]\n",
    "    df = pd.concat([pd.DataFrame(X_reduced, columns=selected_features), df[target]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1166a",
   "metadata": {},
   "source": [
    "# Q4  Parse the Json and make the model objects (using sklean) that can handle what is required in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified). Keep in mind not to pick models that don’t apply for the prediction_type specified\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3819e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Regression\": {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"ElasticNet\": ElasticNet(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "        \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "        \"SVR\": SVR(),\n",
    "        \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "        \"MLPRegressor\": MLPRegressor(),\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor()\n",
    "    },\n",
    "    \"Classification\": {\n",
    "        \"LogisticRegression\": LogisticRegression(),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "        \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "        \"SVC\": SVC(),\n",
    "        \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "        \"MLPClassifier\": MLPClassifier(),\n",
    "        \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "    }\n",
    "}\n",
    "\n",
    "selected_models = []\n",
    "for model_name, model_details in data[\"design_state_data\"][\"algorithms\"].items():\n",
    "    if model_details[\"is_selected\"]:\n",
    "        if model_name in models[prediction_type]:\n",
    "            selected_models.append((model_name, models[prediction_type][model_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca02ac",
   "metadata": {},
   "source": [
    "# Q5 Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, model in selected_models:\n",
    "    pipeline = create_pipeline(model, prediction_type)\n",
    "    hyperparameters = data[\"design_state_data\"][\"hyperparameters\"]\n",
    "    grid_search = GridSearchCV(pipeline, hyperparameters, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "    grid_search.fit(df.drop(columns=[target]), df[target])\n",
    "\n",
    "    y_true = df[target]\n",
    "    y_pred = grid_search.predict(df.drop(columns=[target]))\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_,\n",
    "        **get_model_metrics(y_true, y_pred, prediction_type)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca291ac",
   "metadata": {},
   "source": [
    "# obtaining required metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faadf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {metrics['best_params']}\")\n",
    "    print(f\"Best Score (MSE): {metrics['best_score']}\")\n",
    "    print(f\"Mean Squared Error: {metrics['mean_squared_error']}\")\n",
    "    print(f\"Mean Absolute Error: {metrics['mean_absolute_error']}\")\n",
    "    print(f\"R-squared Score: {metrics['r2_score']}\")\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6df4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
